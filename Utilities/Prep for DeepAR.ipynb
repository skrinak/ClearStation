{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c anaconda pandas-datareader -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_name = 'sparkSummitDemo'\n",
    "start_date = '2014-01-01'\n",
    "end_date = '2016-01-01'\n",
    "\n",
    "start_date_full = start_date + ' 00:00:00'\n",
    "end_date_full = end_date + ' 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "s3_bucket = '2021-demos'\n",
    "s3_prefix = 'sparkSummit'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# role = \"your-sagemaker-role-arn\"\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol  = web.DataReader('SPY', data_source = 'yahoo', start=start_date, end=end_date)\n",
    "symbol0 = web.DataReader('AMZN', data_source = 'yahoo', start=start_date, end=end_date)\n",
    "symbol1 = web.DataReader('GOOG', data_source = 'yahoo', start=start_date, end=end_date)\n",
    "symbol2 = web.DataReader('AAPL', data_source = 'yahoo', start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rows = round(symbol.shape[0]*.8)\n",
    "test_rows = symbol.shape[0] - training_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": start_date_full,\n",
    "        \"target\": symbol[:training_rows].Close.tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in symbol\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"start\": start_date_full,\n",
    "        \"target\": symbol[:test_rows].Close.tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in symbol\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "for dataset in ['train', 'test']:\n",
    "    filename = dataset + '.json'\n",
    "    with open(filename, 'rb') as data:\n",
    "        key = s3_prefix + '/data/' + dataset + '/' + filename\n",
    "        result = bucket.put_object(Key=key, Body=data)\n",
    "        print('Uploaded file to {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for 14 days\n",
    "prediction_length = 14\n",
    "\n",
    "# Use 14 days as context length, \n",
    "# This is the number of state updates accomplished before making predictions\n",
    "context_length = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": 'D',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to deployment we need to create a class that handles I/O with the following functions: \n",
    "\n",
    "* set_prediction_parameters() - set frequency of incoming data and length to predict\n",
    "* __encode_request() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_obj(ts, cat=None, dynamic_feat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat \n",
    "    return obj\n",
    "\n",
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=test_rows, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = symbol.Close\n",
    "test_data0 = symbol0.Close\n",
    "test_data1 = symbol1.Close\n",
    "test_data2 = symbol2.Close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_portfolio = [test_data, test_data0, test_data1, test_data2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=test_portfolio[0], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
